{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9xtNFHQGVGeB3bdQXZIKn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Спочатку імпортуємо бібліотеку"],"metadata":{"id":"BjUAdWpcHaji"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"6BEg3N1nHK_r","executionInfo":{"status":"ok","timestamp":1724070867825,"user_tz":-60,"elapsed":3,"user":{"displayName":"Dmytro Umanskyi","userId":"08552076404851829492"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["# Далі ініціалізуємо параметри\n","Ваги та зсуви (biases). Входи будуть передаватись через ці ваги та зсуви для отримання вихідного значення.\n","\n","* `n_inputs` - кількість входів (feature dimension)\n","* `n_neurons` - кількість нейронів у шарі\n"],"metadata":{"id":"JofovwBtHhOX"}},{"cell_type":"code","source":["n_inputs = 3\n","n_neurons = 4\n","\n","# ініціалізація ваг\n","weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n","\n","# ініціалізація зсувів (biases)\n","biases = np.zeros((1, n_neurons))\n","\n","print(\"Weights:\\n\", weights)\n","print(\"Biases:\\n\", biases)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nqz8IpoTHXry","executionInfo":{"status":"ok","timestamp":1724070871700,"user_tz":-60,"elapsed":284,"user":{"displayName":"Dmytro Umanskyi","userId":"08552076404851829492"}},"outputId":"3735d455-0387-4f5f-867d-59ccc627b516"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Weights:\n"," [[-0.01828668  0.00942478 -0.01208182  0.00639839]\n"," [ 0.00856773  0.00131158 -0.00384495  0.00379481]\n"," [ 0.00865689 -0.00230207 -0.00231195  0.0055007 ]]\n","Biases:\n"," [[0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["# Пряме поширення (forward pass)\n","\n","Тепер, коли ваги та зсуви ініціалізовані, можна здійснити пряме поширення, що означає розрахунок виходу для певного входу.\n","\n","* Вхідні дані (`inputs`) можуть бути або задані вручну, або отримані з реального набору даних. Для прикладу буде створено випадкові дані."],"metadata":{"id":"IGmfwK4aJIb7"}},{"cell_type":"code","source":["# припустимо, що є 2 приклади вхідних даних\n","inputs = np.random.randn(2, n_inputs)\n","\n","\n","# вихід шару нейронної мережі = XW + b\n","layer_output = np.dot(inputs, weights) + biases\n","\n","print(\"Layer output before activation:\\n\", layer_output)\n","print(inputs.shape, weights.shape, layer_output.shape, biases.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVurMhzDJnpM","executionInfo":{"status":"ok","timestamp":1724070881968,"user_tz":-60,"elapsed":288,"user":{"displayName":"Dmytro Umanskyi","userId":"08552076404851829492"}},"outputId":"8b6d57ef-da19-4db7-dfcc-150121456631"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer output before activation:\n"," [[-0.01626915  0.01266822 -0.02042697  0.01422049]\n"," [ 0.02572744 -0.01219034  0.01411903 -0.00625193]]\n","(2, 3) (3, 4) (2, 4) (1, 4)\n"]}]},{"cell_type":"markdown","source":["$X$ - це матриця входів, $W$ - матриця ваг, $b$ - матриця зсувів (biases)"],"metadata":{"id":"CFpmbLoiK4oo"}},{"cell_type":"markdown","source":["# Активаційна функція ReLU\n","\n","До вихідного шару буде застосовано функцію активації ReLU (Rectified Linear Unit), що перетворює всі негативні значення на нулі, що додає нелінійності моделі"],"metadata":{"id":"otl0V62CLqdF"}},{"cell_type":"code","source":["# реалізація функції ReLU\n","def relu(x):\n","  return np.maximum(0, x)\n","\n","# застосування ReLU до вихідного шару\n","output = relu(layer_output)\n","\n","print(\"Layer output after ReLU activation:\\n\", output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AOJJI0heMBXn","executionInfo":{"status":"ok","timestamp":1724070919168,"user_tz":-60,"elapsed":257,"user":{"displayName":"Dmytro Umanskyi","userId":"08552076404851829492"}},"outputId":"21c7cd39-8fab-4eca-ddbe-574e7ae00cd2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer output after ReLU activation:\n"," [[0.         0.01266822 0.         0.01422049]\n"," [0.02572744 0.         0.01411903 0.        ]]\n"]}]},{"cell_type":"markdown","source":["# Реалізація функції зворотного поширення (backward pass)\n","\n","Щоб зробити навчання нейронної мережі можливим, потрібно реалізувати зворотне поширення. Проте, оскільки мета цього проєкту - прости шар, то обчислимо лише градієнти для ваг та зсувів"],"metadata":{"id":"Yy2X3fu-Mrdv"}},{"cell_type":"code","source":["# приклад градієнту для вихідного шару (одиничні градієнти для прикладу)\n","dvalues = np.ones_like(output)\n","\n","# градієнт ReLU для входу\n","drelu = dvalues.copy()\n","drelu[layer_output <= 0] = 0\n","# drelu, layer_output <= 0\n","\n","# градієнт щодо ваг\n","dweights = np.dot(inputs.T, drelu)\n","# градієнт щодо зсувів\n","dbiases = np.sum(drelu, axis=0, keepdims=True)\n","# градієнт вихідного шару\n","dinputs = np.dot(drelu, weights.T)\n","\n","print(\"dWeights:\\n\", dweights)\n","print(\"dBiases:\\n\", dbiases)\n","print(\"dInputs:\\n\", dinputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egyH1wGXND5e","executionInfo":{"status":"ok","timestamp":1724070921693,"user_tz":-60,"elapsed":225,"user":{"displayName":"Dmytro Umanskyi","userId":"08552076404851829492"}},"outputId":"1c2cdfac-ac4f-4c35-eb6e-10c89b93d6f0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["dWeights:\n"," [[-1.24575204  1.41149282 -1.24575204  1.41149282]\n"," [ 0.0931002   0.53002041  0.0931002   0.53002041]\n"," [ 0.24825473  0.57772328  0.24825473  0.57772328]]\n","dBiases:\n"," [[1. 1. 1. 1.]]\n","dInputs:\n"," [[ 0.01582317  0.00510639  0.00319863]\n"," [-0.0303685   0.00472278  0.00634495]]\n"]}]},{"cell_type":"markdown","source":["# Оновлення параметрів\n","\n","Останній крок - оновлення ваг та зсувів та основі обчислених градієнтів. Це основа процесу навчання нейронної мережі"],"metadata":{"id":"s2uVtBHKO7xo"}},{"cell_type":"code","source":["learning_rate = 0.001\n","\n","# оновлення ваг та зсувів\n","weights -= learning_rate * dweights\n","biases -= learning_rate * dbiases\n","\n","print(\"Updated Weights:\\n\", weights)\n","print(\"Updated Biases:\\n\", biases)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRHHthBQPIIA","executionInfo":{"status":"ok","timestamp":1724070934780,"user_tz":-60,"elapsed":276,"user":{"displayName":"Dmytro Umanskyi","userId":"08552076404851829492"}},"outputId":"f9cb3d85-549e-4f9d-9401-c0bf4d8d67e3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated Weights:\n"," [[-0.01704092  0.00801329 -0.01083607  0.00498689]\n"," [ 0.00847463  0.00078156 -0.00393805  0.00326479]\n"," [ 0.00840864 -0.0028798  -0.0025602   0.00492298]]\n","Updated Biases:\n"," [[-0.001 -0.001 -0.001 -0.001]]\n"]}]},{"cell_type":"markdown","source":["# Про проект\n","\n","Цей проект демонструє, як реалізувати **базовий повнозв'язний шар** (fully connected layer) нейронної мережі, використовуючи лише бібліотеку NumPy. Такий шар є ключовим будівельним блоком багатьох сучасних нейронних мереж.\n","\n","## Суть проекту\n","1. **Повнозв'язний шар**:\n","\n"," * Повнозв'язний шар отримує на вході вектор (або набір векторів, якщо розглядається кілька прикладів одночасно) та перетворює його на інший вектор з допомогою матричного множення та додавання зсуву (bias). Кожен нейрон у шарі має свої ваги та зсув.\n"," * Вихід цього шару є лінійною комбінацією вхідних даних.\n","\n","2. **Функція активації**:\n","\n"," * Після того, як лінійна комбінація обчислена, на неї застосовується **функція активації** (у цьому проекті це ReLU — Rectified Linear Unit). Функція активації додає нелінійність у модель, що дозволяє нейронній мережі вчитися моделювати складні функції.\n"," * ReLU перетворює всі негативні значення на нулі, залишаючи позитивні значення без змін. Це допомагає нейронній мережі краще відокремлювати класи в задачах класифікації.\n","\n","3. **Зворотне поширення (Backward Pass)**:\n","\n"," * **Зворотне поширення** — це процес, за допомогою якого нейронна мережа вчиться на помилках. На основі різниці між передбаченням і правильними відповідями (похибкою) обчислюються градієнти параметрів (ваг і зсувів).\n"," * Ці градієнти використовуються для коригування параметрів шару так, щоб зменшити похибку на наступних ітераціях.\n","\n","## Що робить цей проект?\n","* **Вхідні дані**: Вектор або набір векторів, що представляють приклади даних.\n","* **Ваги та зсуви**: Параметри шару, які впливають на вихід.\n","* **Вихід**: Новий вектор, отриманий шляхом множення вхідного вектора на ваги та додавання зсуву, до якого застосовується функція активації ReLU.\n","* **Оновлення параметрів**: На основі обчислених градієнтів параметри оновлюються, що дозволяє мережі навчатися.\n","\n","## Для чого це потрібно?\n","* **Розуміння основ**: Проект дозволяє зрозуміти, як працює основний компонент будь-якої нейронної мережі — повнозв'язний шар, і як дані трансформуються на кожному етапі проходження через нього.\n","* **Практика з лінійною алгеброю**: Ви застосовуєте матричне множення, що є основою для роботи з нейронними мережами.\n","* **Базове навчання нейронних мереж**: Хоча ми не реалізували повну нейронну мережу, цей проект дає базове уявлення про те, як відбувається процес навчання, як обчислюються градієнти, і як ці градієнти використовуються для покращення моделі.\n","\n","\n","Таким чином, цей проект дає вам фундаментальні знання та навички, необхідні для побудови складніших моделей машинного навчання."],"metadata":{"id":"13JtKzn_QV4H"}}]}